{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "RGL86JNJCAc-"
      },
      "outputs": [],
      "source": [
        "#1. Import Libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Load and Preprocess the data\n",
        "(X_train, y_train), (X_test, y_test)= mnist.load_data()\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZjiwd-wIeOj",
        "outputId": "ec31934f-3cdc-482e-9fb2-56cbc2e78450"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_input_img(i):\n",
        "    plt.imshow(X_train[0], cmap = 'binary')\n",
        "    plt.title(y_train[i])\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "mlj-0a1zJz5C"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    plot_input_img(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZIAqqmtPKqkB",
        "outputId": "dd76c5f4-9bab-4358-efcd-e706bafde901"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Preprocess the images\n",
        "\n",
        "#Normalizing the image to [0,1] range\n",
        "X_train = X_train.astype('float32')/255\n",
        "X_test = X_test.astype('float32')/255\n",
        "\n",
        "#Reshape the dimensions of images to (28, 28, 1)\n",
        "X_train = np.expand_dims(X_train, -1)\n",
        "X_test = np.expand_dims(X_test, -1)\n",
        "\n",
        "#Convert classes to one hot vector\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train)\n",
        "y_test = keras.utils.to_categorical(y_test)\n"
      ],
      "metadata": {
        "id": "xjUdVrebK86h"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99Jnb3oGVuR3",
        "outputId": "123949a6-dcd1-4170-e96a-d98c47e74a6b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28, 1), (60000, 10), (10000, 28, 28, 1), (10000, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Building the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu', input_shape = (28,28,1)))\n",
        "model.add(MaxPool2D(pool_size = (2,2)))\n",
        "model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu'))\n",
        "model.add(MaxPool2D(pool_size = (2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(10, activation = 'softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLpBFJxOMsP2",
        "outputId": "d230af43-c296-4526-c60c-69245fed518d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPoolin  (None, 13, 13, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPoolin  (None, 5, 5, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                16010     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 34826 (136.04 KB)\n",
            "Trainable params: 34826 (136.04 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4.1 Compile the model\n",
        "model.compile(optimizer= 'adam' , loss = keras.losses.categorical_crossentropy , metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "iK4y7wrfPIlt"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5 Callbacks\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "#EarlyStopping\n",
        "early_stop = EarlyStopping(monitor = 'val_acc',min_delta= 0.01, patience = 4, verbose = 1)\n",
        "\n",
        "#ModelCheckpoint\n",
        "model_checkpoint = ModelCheckpoint('./bestmodel.h5', monitor = 'val_acc', verbose = 1, save_best_only = True)\n",
        "\n",
        "cb = [early_stop, model_checkpoint]\n"
      ],
      "metadata": {
        "id": "Toq-KHghPjav"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Model Training\n",
        "his = model.fit(X_train, y_train, epochs= 50, validation_split= 0.3 , callbacks = cb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQlAMs4wQ4Cn",
        "outputId": "43223c62-7118-4ca0-a2fd-4960154fc409"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1312/1313 [============================>.] - ETA: 0s - loss: 0.1190 - accuracy: 0.9641"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 42s 32ms/step - loss: 0.1190 - accuracy: 0.9641 - val_loss: 0.0700 - val_accuracy: 0.9796\n",
            "Epoch 2/50\n",
            "1313/1313 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9791"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 39s 30ms/step - loss: 0.0668 - accuracy: 0.9791 - val_loss: 0.0526 - val_accuracy: 0.9836\n",
            "Epoch 3/50\n",
            "1312/1313 [============================>.] - ETA: 0s - loss: 0.0516 - accuracy: 0.9835"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 38s 29ms/step - loss: 0.0516 - accuracy: 0.9835 - val_loss: 0.0538 - val_accuracy: 0.9832\n",
            "Epoch 4/50\n",
            "1313/1313 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9871"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 40s 31ms/step - loss: 0.0415 - accuracy: 0.9871 - val_loss: 0.0543 - val_accuracy: 0.9828\n",
            "Epoch 5/50\n",
            "1312/1313 [============================>.] - ETA: 0s - loss: 0.0365 - accuracy: 0.9882"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 40s 31ms/step - loss: 0.0365 - accuracy: 0.9882 - val_loss: 0.0405 - val_accuracy: 0.9875\n",
            "Epoch 6/50\n",
            "1311/1313 [============================>.] - ETA: 0s - loss: 0.0311 - accuracy: 0.9900"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 40s 31ms/step - loss: 0.0311 - accuracy: 0.9900 - val_loss: 0.0420 - val_accuracy: 0.9873\n",
            "Epoch 7/50\n",
            "1312/1313 [============================>.] - ETA: 0s - loss: 0.0282 - accuracy: 0.9914"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 42s 32ms/step - loss: 0.0282 - accuracy: 0.9914 - val_loss: 0.0416 - val_accuracy: 0.9872\n",
            "Epoch 8/50\n",
            "1311/1313 [============================>.] - ETA: 0s - loss: 0.0246 - accuracy: 0.9914"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 40s 31ms/step - loss: 0.0245 - accuracy: 0.9915 - val_loss: 0.0421 - val_accuracy: 0.9877\n",
            "Epoch 9/50\n",
            "1312/1313 [============================>.] - ETA: 0s - loss: 0.0216 - accuracy: 0.9931"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 40s 31ms/step - loss: 0.0216 - accuracy: 0.9931 - val_loss: 0.0401 - val_accuracy: 0.9887\n",
            "Epoch 10/50\n",
            "1312/1313 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9933"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 45s 34ms/step - loss: 0.0201 - accuracy: 0.9933 - val_loss: 0.0419 - val_accuracy: 0.9884\n",
            "Epoch 11/50\n",
            "1311/1313 [============================>.] - ETA: 0s - loss: 0.0190 - accuracy: 0.9935"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 39s 30ms/step - loss: 0.0190 - accuracy: 0.9935 - val_loss: 0.0413 - val_accuracy: 0.9885\n",
            "Epoch 12/50\n",
            "1312/1313 [============================>.] - ETA: 0s - loss: 0.0164 - accuracy: 0.9943"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 40s 30ms/step - loss: 0.0164 - accuracy: 0.9943 - val_loss: 0.0484 - val_accuracy: 0.9873\n",
            "Epoch 13/50\n",
            "1313/1313 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9946"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 40s 31ms/step - loss: 0.0150 - accuracy: 0.9946 - val_loss: 0.0518 - val_accuracy: 0.9888\n",
            "Epoch 14/50\n",
            "1312/1313 [============================>.] - ETA: 0s - loss: 0.0134 - accuracy: 0.9950"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 41s 31ms/step - loss: 0.0134 - accuracy: 0.9950 - val_loss: 0.0450 - val_accuracy: 0.9896\n",
            "Epoch 15/50\n",
            "1312/1313 [============================>.] - ETA: 0s - loss: 0.0137 - accuracy: 0.9951"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 39s 30ms/step - loss: 0.0136 - accuracy: 0.9951 - val_loss: 0.0401 - val_accuracy: 0.9907\n",
            "Epoch 16/50\n",
            "1313/1313 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9956"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 39s 30ms/step - loss: 0.0130 - accuracy: 0.9956 - val_loss: 0.0439 - val_accuracy: 0.9903\n",
            "Epoch 17/50\n",
            "1313/1313 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.9963"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 40s 31ms/step - loss: 0.0107 - accuracy: 0.9963 - val_loss: 0.0453 - val_accuracy: 0.9901\n",
            "Epoch 18/50\n",
            "1312/1313 [============================>.] - ETA: 0s - loss: 0.0117 - accuracy: 0.9962"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 38s 29ms/step - loss: 0.0117 - accuracy: 0.9962 - val_loss: 0.0440 - val_accuracy: 0.9898\n",
            "Epoch 19/50\n",
            "1313/1313 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9965"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 39s 30ms/step - loss: 0.0100 - accuracy: 0.9965 - val_loss: 0.0480 - val_accuracy: 0.9895\n",
            "Epoch 20/50\n",
            "1311/1313 [============================>.] - ETA: 0s - loss: 0.0096 - accuracy: 0.9966"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 41s 31ms/step - loss: 0.0096 - accuracy: 0.9966 - val_loss: 0.0432 - val_accuracy: 0.9906\n",
            "Epoch 21/50\n",
            "1313/1313 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.9964"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 40s 30ms/step - loss: 0.0103 - accuracy: 0.9964 - val_loss: 0.0551 - val_accuracy: 0.9897\n",
            "Epoch 22/50\n",
            "1311/1313 [============================>.] - ETA: 0s - loss: 0.0092 - accuracy: 0.9969"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 39s 30ms/step - loss: 0.0092 - accuracy: 0.9969 - val_loss: 0.0429 - val_accuracy: 0.9909\n",
            "Epoch 23/50\n",
            "1311/1313 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9974"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 39s 30ms/step - loss: 0.0078 - accuracy: 0.9974 - val_loss: 0.0482 - val_accuracy: 0.9907\n",
            "Epoch 24/50\n",
            "1311/1313 [============================>.] - ETA: 0s - loss: 0.0088 - accuracy: 0.9970"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 40s 30ms/step - loss: 0.0088 - accuracy: 0.9970 - val_loss: 0.0503 - val_accuracy: 0.9902\n",
            "Epoch 25/50\n",
            "1313/1313 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9970"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 39s 30ms/step - loss: 0.0091 - accuracy: 0.9970 - val_loss: 0.0541 - val_accuracy: 0.9892\n",
            "Epoch 26/50\n",
            "1313/1313 [==============================] - ETA: 0s - loss: 0.0069 - accuracy: 0.9978"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 39s 30ms/step - loss: 0.0069 - accuracy: 0.9978 - val_loss: 0.0540 - val_accuracy: 0.9901\n",
            "Epoch 27/50\n",
            "1311/1313 [============================>.] - ETA: 0s - loss: 0.0092 - accuracy: 0.9970"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 45s 34ms/step - loss: 0.0092 - accuracy: 0.9970 - val_loss: 0.0566 - val_accuracy: 0.9892\n",
            "Epoch 28/50\n",
            "1312/1313 [============================>.] - ETA: 0s - loss: 0.0067 - accuracy: 0.9976"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 39s 30ms/step - loss: 0.0067 - accuracy: 0.9976 - val_loss: 0.0556 - val_accuracy: 0.9897\n",
            "Epoch 29/50\n",
            "1313/1313 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9977"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 39s 30ms/step - loss: 0.0073 - accuracy: 0.9977 - val_loss: 0.0542 - val_accuracy: 0.9901\n",
            "Epoch 30/50\n",
            "1313/1313 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9970"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 40s 31ms/step - loss: 0.0086 - accuracy: 0.9970 - val_loss: 0.0501 - val_accuracy: 0.9897\n",
            "Epoch 31/50\n",
            "1312/1313 [============================>.] - ETA: 0s - loss: 0.0069 - accuracy: 0.9976"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 37s 29ms/step - loss: 0.0069 - accuracy: 0.9976 - val_loss: 0.0481 - val_accuracy: 0.9903\n",
            "Epoch 32/50\n",
            "1312/1313 [============================>.] - ETA: 0s - loss: 0.0069 - accuracy: 0.9977"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 39s 29ms/step - loss: 0.0069 - accuracy: 0.9977 - val_loss: 0.0488 - val_accuracy: 0.9906\n",
            "Epoch 33/50\n",
            "1312/1313 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9984"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 40s 31ms/step - loss: 0.0048 - accuracy: 0.9984 - val_loss: 0.0592 - val_accuracy: 0.9898\n",
            "Epoch 34/50\n",
            "1312/1313 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9975"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 40s 31ms/step - loss: 0.0079 - accuracy: 0.9975 - val_loss: 0.0562 - val_accuracy: 0.9908\n",
            "Epoch 35/50\n",
            "1311/1313 [============================>.] - ETA: 0s - loss: 0.0062 - accuracy: 0.9980"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 39s 30ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.0527 - val_accuracy: 0.9904\n",
            "Epoch 36/50\n",
            "1312/1313 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9976"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 39s 30ms/step - loss: 0.0065 - accuracy: 0.9976 - val_loss: 0.0553 - val_accuracy: 0.9903\n",
            "Epoch 37/50\n",
            "1312/1313 [============================>.] - ETA: 0s - loss: 0.0060 - accuracy: 0.9981"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 40s 31ms/step - loss: 0.0060 - accuracy: 0.9981 - val_loss: 0.0572 - val_accuracy: 0.9907\n",
            "Epoch 38/50\n",
            "1313/1313 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9977"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 44s 33ms/step - loss: 0.0063 - accuracy: 0.9977 - val_loss: 0.0512 - val_accuracy: 0.9907\n",
            "Epoch 39/50\n",
            "1312/1313 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9980"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 40s 30ms/step - loss: 0.0061 - accuracy: 0.9980 - val_loss: 0.0521 - val_accuracy: 0.9909\n",
            "Epoch 40/50\n",
            "1311/1313 [============================>.] - ETA: 0s - loss: 0.0063 - accuracy: 0.9979"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 45s 35ms/step - loss: 0.0063 - accuracy: 0.9979 - val_loss: 0.0643 - val_accuracy: 0.9888\n",
            "Epoch 41/50\n",
            "1311/1313 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9980"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 40s 30ms/step - loss: 0.0054 - accuracy: 0.9980 - val_loss: 0.0613 - val_accuracy: 0.9899\n",
            "Epoch 42/50\n",
            "1313/1313 [==============================] - ETA: 0s - loss: 0.0046 - accuracy: 0.9985"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 40s 31ms/step - loss: 0.0046 - accuracy: 0.9985 - val_loss: 0.0595 - val_accuracy: 0.9901\n",
            "Epoch 43/50\n",
            "1311/1313 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9981"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 42s 32ms/step - loss: 0.0053 - accuracy: 0.9981 - val_loss: 0.0561 - val_accuracy: 0.9908\n",
            "Epoch 44/50\n",
            "1311/1313 [============================>.] - ETA: 0s - loss: 0.0050 - accuracy: 0.9980"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 40s 30ms/step - loss: 0.0050 - accuracy: 0.9980 - val_loss: 0.0604 - val_accuracy: 0.9907\n",
            "Epoch 45/50\n",
            "1312/1313 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9980"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 40s 30ms/step - loss: 0.0065 - accuracy: 0.9980 - val_loss: 0.0593 - val_accuracy: 0.9887\n",
            "Epoch 46/50\n",
            "1312/1313 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9984"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 41s 31ms/step - loss: 0.0048 - accuracy: 0.9984 - val_loss: 0.0575 - val_accuracy: 0.9907\n",
            "Epoch 47/50\n",
            "1311/1313 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9987"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 40s 30ms/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.0553 - val_accuracy: 0.9908\n",
            "Epoch 48/50\n",
            "1313/1313 [==============================] - ETA: 0s - loss: 0.0055 - accuracy: 0.9980"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 41s 32ms/step - loss: 0.0055 - accuracy: 0.9980 - val_loss: 0.0598 - val_accuracy: 0.9906\n",
            "Epoch 49/50\n",
            "1313/1313 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 0.9979"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 39s 30ms/step - loss: 0.0059 - accuracy: 0.9979 - val_loss: 0.0575 - val_accuracy: 0.9907\n",
            "Epoch 50/50\n",
            "1311/1313 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9983"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1313/1313 [==============================] - 41s 31ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.0531 - val_accuracy: 0.9907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_S = keras.models.load_model(\"C://Users//PC//OneDrive//Documents//dumps//bestmodel.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "eYchr-sszkwi",
        "outputId": "21028fee-d8eb-4bc6-ff43-56e36d421f1b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "C://Users//PC//OneDrive//Documents//dumps//bestmodel.h5; No such file or directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-dea221cb06fe>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_S\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C://Users//PC//OneDrive//Documents//dumps//bestmodel.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# Copy from remote to temporary local directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;31m# Switch filepath to local zipfile for loading model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mcopy_v2\u001b[0;34m(src, dst, overwrite)\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m   \"\"\"\n\u001b[0;32m--> 581\u001b[0;31m   _pywrap_file_io.CopyFile(\n\u001b[0m\u001b[1;32m    582\u001b[0m       compat.path_to_bytes(src), compat.path_to_bytes(dst), overwrite)\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: C://Users//PC//OneDrive//Documents//dumps//bestmodel.h5; No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1qPWgR6-2-Wx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}